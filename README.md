<p align="center">
  <img src="./img.png" alt="Project Banner" width="100%">
</p>

# [AI_Assistance_for_blinds] üéØ

## Basic Details

### Team Name: [Wildx]

### Team Members
- Member 1:[Ann Rose Mathew] - [LBS Institute of Technology for Women]
- Member 2: [Ann Mariya Joju M] - [LBS Institute of Technology for Women]

### Hosted Project Link
[https://github.com/annrose2277-glitch/AI_Assistance_for_blinds.git]

### Project Description
[This project modernizes traditional navigational aids by integrating computer vision and sensor fusion into an intelligent assistive device. By detecting objects in real-time and converting visual data into auditory feedback via text-to-speech, we provide visually impaired users to enable greater independence and confidence in navigation.]

### The Problem statement
[Visually impaired individuals face significant barriers to independent mobility and experience social anxiety due to a reliance on sighted assistance. This dependency is primarily caused by the limitations of traditional white canes, which provide only tactile, ground-level feedback and lack the capability to detect mid-air obstacles, identify specific objects, or provide the environmental context necessary for safe, confident navigation.]

### The Solution
[Our prototype transforms the standard white cane into an intelligent navigation assistant. By mounting a lightweight webcam and an AI-processing unit onto the cane

Computer Vision (The Eyes): We utilize the YOLO (You Only Look Once) model. Unlike traditional models, YOLO scans the entire camera frame in one pass, making it incredibly fast for real-time detection.

The "40% Rule" (Smart Filtering): To prevent the user from being overwhelmed by too much information, the system only triggers a high-priority alert when an object‚Äôs bounding box covers 40% of the camera's view. This ensures we are reporting the most relevant, immediate obstacles.

Voice Interface (The Guide): Using the Pyttsx3 library, the system converts the AI's visual data into clear speech. Instead of a beep, the user hears: "An object is in front of you: [Object Name]."]

---

## Technical Details

### Technologies/Components Used

**For Software:**
- Languages used: [Python]
- Frameworks used: [Ultralytics YOLO model YOLO26n]
- Libraries used: [OpenCV,Pyttsx3,Threading,NumPy,Time]
- Tools used: [Pip,Webcam Hardware,Integrated Development Environment (IDE)]

**For Hardware:**
- Main components: [List main components]
- Specifications: [Technical specifications]
- Tools required: [List tools needed]

---

## Features

List the key features of your project:
- Feature 1: [Real-Time          --- Detection	Instant identification of obstacles as the user walks.]
- Feature 2: [Proximity Awareness ---	The 40% threshold ensures users are warned about objects directly in their path.]
- Feature 3: [Natural Language Feedback ---	Uses human-like speech to describe the environment, reducing the learning curve.]
- Feature 4: [Visual Monitoring ---	Provides a live feed with bounding boxes on a display for caregivers or for calibration.]

---

## Implementation

### For Software:

#### Installation
```bash
[pip install ultralytics opencv-python pyttsx3 numpy]
```

#### Run
```bash
[python main.py]
```

### For Hardware:

#### Components Required
[List all components needed with specifications]

#### Circuit Setup
[Explain how to set up the circuit]

---

## Project Documentation

### For Software:

#### Screenshots (Add at least 3)

![Screenshot1](Add screenshot 1 here with proper name)
*Add caption explaining what this shows*

![Screenshot2](Add screenshot 2 here with proper name)
*Add caption explaining what this shows*

![Screenshot3](Add screenshot 3 here with proper name)
*Add caption explaining what this shows*

#### Diagrams

**System Architecture:**

![Architecture Diagram](docs/architecture.png)
*Explain your system architecture - components, data flow, tech stack interaction*

**Application Workflow:**

![Workflow](docs/workflow.png)
*Add caption explaining your workflow*

---

### For Hardware:

#### Schematic & Circuit

![Circuit](Add your circuit diagram here)
*Add caption explaining connections*

![Schematic](Add your schematic diagram here)
*Add caption explaining the schematic*

#### Build Photos

![Team](Add photo of your team here)

![Components](Add photo of your components here)
*List out all components shown*

![Build](Add photos of build process here)
*Explain the build steps*

![Final](Add photo of final product here)
*Explain the final build*

---

## Additional Documentation

### For Web Projects with Backend:

#### API Documentation

**Base URL:** `https://api.yourproject.com`

##### Endpoints

**GET /api/endpoint**
- **Description:** [What it does]
- **Parameters:**
  - `param1` (string): [Description]
  - `param2` (integer): [Description]
- **Response:**
```json
{
  "status": "success",
  "data": {}
}
```

**POST /api/endpoint**
- **Description:** [What it does]
- **Request Body:**
```json
{
  "field1": "value1",
  "field2": "value2"
}
```
- **Response:**
```json
{
  "status": "success",
  "message": "Operation completed"
}
```

[Add more endpoints as needed...]

---

### For Mobile Apps:

#### App Flow Diagram

![App Flow](docs/app-flow.png)
*Explain the user flow through your application*

#### Installation Guide

**For Android (APK):**
1. Download the APK from [Release Link]
2. Enable "Install from Unknown Sources" in your device settings:
   - Go to Settings > Security
   - Enable "Unknown Sources"
3. Open the downloaded APK file
4. Follow the installation prompts
5. Open the app and enjoy!

**For iOS (IPA) - TestFlight:**
1. Download TestFlight from the App Store
2. Open this TestFlight link: [Your TestFlight Link]
3. Click "Install" or "Accept"
4. Wait for the app to install
5. Open the app from your home screen

**Building from Source:**
```bash
# For Android
flutter build apk
# or
./gradlew assembleDebug

# For iOS
flutter build ios
# or
xcodebuild -workspace App.xcworkspace -scheme App -configuration Debug
```

---

### For Hardware Projects:

#### Bill of Materials (BOM)

| Component | Quantity | Specifications | Price | Link/Source |
|-----------|----------|----------------|-------|-------------|
| Arduino Uno | 1 | ATmega328P, 16MHz | ‚Çπ450 | [Link] |
| LED | 5 | Red, 5mm, 20mA | ‚Çπ5 each | [Link] |
| Resistor | 5 | 220Œ©, 1/4W | ‚Çπ1 each | [Link] |
| Breadboard | 1 | 830 points | ‚Çπ100 | [Link] |
| Jumper Wires | 20 | Male-to-Male | ‚Çπ50 | [Link] |
| [Add more...] | | | | |

**Total Estimated Cost:** ‚Çπ[Amount]

#### Assembly Instructions

**Step 1: Prepare Components**
1. Gather all components listed in the BOM
2. Check component specifications
3. Prepare your workspace
![Step 1](images/assembly-step1.jpg)
*Caption: All components laid out*

**Step 2: Build the Power Supply**
1. Connect the power rails on the breadboard
2. Connect Arduino 5V to breadboard positive rail
3. Connect Arduino GND to breadboard negative rail
![Step 2](images/assembly-step2.jpg)
*Caption: Power connections completed*

**Step 3: Add Components**
1. Place LEDs on breadboard
2. Connect resistors in series with LEDs
3. Connect LED cathodes to GND
4. Connect LED anodes to Arduino digital pins (2-6)
![Step 3](images/assembly-step3.jpg)
*Caption: LED circuit assembled*

**Step 4: [Continue for all steps...]**

**Final Assembly:**
![Final Build](images/final-build.jpg)
*Caption: Completed project ready for testing*

---

### For Scripts/CLI Tools:

#### Command Reference

**Basic Usage:**
```bash
python script.py [options] [arguments]
```

**Available Commands:**
- `command1 [args]` - Description of what command1 does
- `command2 [args]` - Description of what command2 does
- `command3 [args]` - Description of what command3 does

**Options:**
- `-h, --help` - Show help message and exit
- `-v, --verbose` - Enable verbose output
- `-o, --output FILE` - Specify output file path
- `-c, --config FILE` - Specify configuration file
- `--version` - Show version information

**Examples:**

```bash
# Example 1: Basic usage
python script.py input.txt

# Example 2: With verbose output
python script.py -v input.txt

# Example 3: Specify output file
python script.py -o output.txt input.txt

# Example 4: Using configuration
python script.py -c config.json --verbose input.txt
```

#### Demo Output

**Example 1: Basic Processing**

**Input:**
```
This is a sample input file
with multiple lines of text
for demonstration purposes
```

**Command:**
```bash
python script.py sample.txt
```

**Output:**
```
Processing: sample.txt
Lines processed: 3
Characters counted: 86
Status: Success
Output saved to: output.txt
```

**Example 2: Advanced Usage**

**Input:**
```json
{
  "name": "test",
  "value": 123
}
```

**Command:**
```bash
python script.py -v --format json data.json
```

**Output:**
```
[VERBOSE] Loading configuration...
[VERBOSE] Parsing JSON input...
[VERBOSE] Processing data...
{
  "status": "success",
  "processed": true,
  "result": {
    "name": "test",
    "value": 123,
    "timestamp": "2024-02-07T10:30:00"
  }
}
[VERBOSE] Operation completed in 0.23s
```

---

## Project Demo

### Video
[Add your demo video link here - YouTube, Google Drive, etc.]

*Explain what the video demonstrates - key features, user flow, technical highlights*

### Additional Demos
[Add any extra demo materials/links - Live site, APK download, online demo, etc.]

---

## AI Tools Used (Optional - For Transparency Bonus)

If you used AI tools during development, document them here for transparency:

**Tool Used:** [Google Gemini]

**Purpose:** [What you used it for]
-for getting prompt,code

**Key Prompts Used:**
-act as an expert prompt engineer
-structured prompt for gemini cli
**Percentage of AI-generated code:** [60%]


**Human Contributions:**
-idea and planning

*Note: Proper documentation of AI usage demonstrates transparency and earns bonus points in evaluation!*

---

## Team Contributions

- [Name 1]: [Specific contributions - e.g., Frontend development, API integration, etc.]
- [Name 2]: [Specific contributions - e.g., Backend development, Database design, etc.]
- [Name 3]: [Specific contributions - e.g., UI/UX design, Testing, Documentation, etc.]

---

## License

This project is licensed under the [LICENSE_NAME] License - see the [LICENSE](LICENSE) file for details.

**Common License Options:**
- MIT License (Permissive, widely used)
- Apache 2.0 (Permissive with patent grant)
- GPL v3 (Copyleft, requires derivative works to be open source)

---

Made with ‚ù§Ô∏è at TinkerHub
